# dataset generation from `convml_tt`

Datasets for `convml_tt` can be generated by describing a "data-source", which
in a unified way allows for the ingestion of data from different sources. As
well as defining the "source" and "product" of your data-source, you can define
the domain and time-span over which to sample data, the number and size of any
triplets you wish to generate and the sampling resolution. These parameters are
all defined through a single `meta.yaml`-file which describes the entire
dataset. An example of a complete configuration-file is given below:

```yaml
source: goes16
product: truecolor_rgb

time:
  t_start: 2020-02-02T14:00
  t_end: 2020-02-02T14:30

domain:
  central_latitude: 13.3 # 13 + (18 / 60)
  central_longitude: -57.72 # -(57 + (43 / 60))
  l_zonal: 200000
  l_meridional: 200000

sampling:
  resolution: 1000.0
  triplets:
    scene_collections_splitting: random_by_relative_sample_size
    N_triplets: {train: 10, study: 2}
    tile_N: 256
```

## The `source` and `product` of your dataset

The first step is to define the `source` and `product`, currently `source in
[goes16, LES]` are supported.


### `goes16` data source

```yaml
source: goes16
product: truecolor_rgb
```

`source == goes16` will automatically fetch
GOES-16 data from Amazon S3 and with `product==truecolor_rgb` will generate
truecolor RGB composites from the first three radiance channels. has been
implemented.

In addition to the `truecolor_rgb` product there are two other sets of source
products that can be defined. `singlechannel` makes it possible
to extract just a single channel, for example `singlechannel__10` will simply
fetch channel 10. You can optionally prefix the channel number by `bt` to
convert radiances to brightness temperature, e.g. `singlechannel__bt_13` will
use channel 13 converted from radiances to brightness temperature

Finally, you can also request GOES-16 derived products, for example `ACHA`
(cloud-top height).

### `LES` data source

```yaml
source: LES
product: singlechannel__{field_name1}
```

`source == LES` is intended to be used for working with data from Large-Eddy
Simulations or other gridded data in netCDF files. In this case any files
residing in `source_data/LES` be used to generate scenes. The `scene_id` is
generated by using the time-coordinate (assumed to be named `time`) of the
data. `product` in this case simply refers to the name of the variable in the
datasets that will be used (support for multi-channel datasets will be added in
future)

```yaml
source: LES
product: singlechannel__rlut
```

Will create single-channel tiles using the `rlut` field in the provided netCDF
file(s). **NOTE**: for now grey-scale images are created from this single field
which are then fed to the neural network as RGB images. In future it will be
possible to define a userfunction which sets up the channels and input
normalisation for the neural network.


### `ceres` source

Fetches GOES-16 CERES products from the SatCORPS team


```yaml
source: ceres
product: {satellite}__{variable_name}
```

For example cloud-top temperature from the GOES-16 satellite

```yaml
source: ceres
product: goes16n__cloud_top_temperature
```

Or the top-of-atmosphere net radiation flux (derived from LW-flux and SW albedo)

```yaml
source: ceres
product: goes16n__approximate_toa_net_radiation_flux
```

The full set of variables from the `ceres` source is:  '`reflectance_vis`,
`visible_count`, `reflec_vis_137`, `temperature_sir`, `temperature_67`,
`temperature_ir`, `temperature_sw`, `broadband_shortwave_albedo`,
`broadband_longwave_flux`, `cloud_ir_emittance`, `cloud_phase`,
`cloud_visible_optical_depth`, `cloud_particle_size`, `cloud_lwp_iwp`,
`cloud_effective_temperature`, `cloud_top_pressure`,
`cloud_effective_pressure`, `cloud_bottom_pressure`, `cloud_top_height`,
`cloud_effective_height`, `cloud_bottom_height`, `cloud_top_temperature`,
`cloud_bottom_temperature`, `pixel_skin_temperature`, `pixel_sza`, `pixel_vza`,
`pixel_aza`, `visible_count_08`, `visible_count_16`, `visible_count_23`,
`temperature_87` and `temperature_133`', including `broadband_shortwave_flux`
derived from `broadband_shortwave_albedo` and
`approximate_toa_net_radiation_flux` from `broadband_longwave_flux` and
`broadband_shortwave_flux`.


## `era5` source

> NB: currently you will need access to JASMIN to be able to use ERA5
> renanalysis data with convml-data


```yaml
source: era5
product: {variable_name}
```

Available variables, 3D: `q`, `t`, `u`, `v`, `rh`, `p` and `alt`, 2D: `z_lcl`,
`tpw`, `bl_umag`, `cl_umag`, `bl_qmean` and `cl_qmean`.


## The timespan of your data-source

In the `time` section of `meta.yaml` you can define the timespan over which you
want to use data. In case of data from the `goes16` source this will be used
both for fetching the correct data and generating scene IDs from this data.
When using a `LES` source this will simply filter scenes from the netCDF files
found. This section can be omitted for `LES` source data, but not for `goes16`
data.

The timespan can be simply defined using `t_start` and `t_end`

```yaml
time:
  t_start: 2020-02-02T14:00
  t_end: 2020-02-02T14:30
```

You can use multiple intervals:

```yaml
time:
  intervals:
    - t_start: 2018-10-01 00:00:00
      t_end: 2019-03-01 00:00:00
    - t_start: 2019-10-01 00:00:00
      t_end: 2020-03-01 00:00:00
    - t_start: 2020-10-01 00:00:00
      t_end: 2021-03-01 00:00:00
  filters:
    N_hours_from_zenith: 4.0
```

And you may optionally list filters to use to only include times match the
filter arguments. You can filter on any `datetime` compent (for example the
`hour`, `minute`, `day`) and there's a specialised filter option called
`N_hours_from_zenith` which calculates the zenith time in the center of the
domain and filters based on time-difference to this time). For example to
select all times on and half-past the hour a maximum of 4 hours from zenith on
the 2/2/2020 you would use the following:

```yaml
time:
  t_start: 2020-02-02T00:00
  t_end: 2020-02-03T00:00
  filters:
    N_hours_from_zenith: 4.0
    minutes: [0, 30]
```


## The sampling `domain` for your data-source

The `domain` section of `meta.yaml` defines the spatial domain within which you
want to use data. If your data is given at lat/lon coordinates it will be
resampled onto a isometric Cartesian grid (by setting `kind == rect`). This is
done by doing a projection onto a tangential plane centered on a point you
define. For `LES` data the `domain` section may be omitted if the whole domain
is to be used (in the case the domain centre will be use for the Cartesian
projection origin).

```yaml
domain:
  central_latitude: 14.0
  central_longitude: -48.0
  l_zonal: 3000.0e+3
  l_meridional: 1000.0e+3
```


## Sampling triplets and the entire domain

To do the tile-based embedding projections using `convml_tt` we must either
feed it individual tiles. There is currently implemented support for producing
triplets of tiles for this purpose or regridding the entire domain at a fixed
resolution and feeding data using a sliding-window to
generate tiles at inference time. In both cases the resolution must be defined
(which is assumed to be given in meters).

```yaml
sampling:
  resolution: 1000.0
  triplets:
    scene_collections_splitting: random_by_relative_sample_size
    N_triplets: {train: 10, study: 2}
    tile_N: 256
```

## Auxiliary data

For some sources (for example `goes16`) there may be auxiliary data fields that
you would like to download and regrid to use during inference time. You should
define the ones you wish to download using the `aux_products` part of the
`meta.yaml` file. This done through creating named groups in the `aux_products`
section. In the example the cloud-top height product from two different sources
will be fetched and can be regridded onto the domain (and any tiles you might
want to sample):

```yaml
aux_products:
  cloud_top_height_goes16:
    source: goes16
    product: ACHA
  cloud_top_height_ceres:
    source: ceres
    product: goes16n__cloud_top_height
    scene_mapping_strategy: all_scenes_within_dt_aux
    dt_aux: PT30M
```

`scene_mapping_strategy` allows you to control how the times for which aux data
is available is mapped to scenes of the primary data source. Currently the
options implemented are `single_scene_per_aux_time` which ensures aux data is
only used for one source scene ID and `all_scenes_within_dt_aux` which ensures
all source scene IDs have a aux data associated (if the scene is within
`dt_aux/2` of the aux scene). The default is `single_scene_per_aux_time`. If
the aux source time resolution isn't provided (`dt_aux`) then it will be
inferred as the smallest time span between any two aux source-files.

To produce regridded data on the whole domain or for tiles the argument
`--aux-name <aux_product_identifier>` can be added to the pipeline commands
given below.

## user-defined functions for image creation

When producing images for model training you may want to define your own
transformations for how the input data scalar fields are turned into the red,
green and blue image channels (for now only RGB image based training data is
supported, this will change in future). This can be achieved by setting the
`product` to `user_function` for a product you are including for creation. As well
as the `product` you will need to define the `input` to the function. For now only
data from `GOES-16` is supported and the inputs should either be a list of
channels or a dictionary with the units of each channel used.

```yaml
aux_products:
  ir_shallow_clouds_ch11_ch14_ch15_v1:
    source: goes16
    product: user_function
    input: [bt_11, bt_14, bt_15]
    image_function: default
```

Second you will need to create a function matching the product name (here
`ir_shallow_clouds_ch11_ch14_ch15_v1`) in a file called `user_functions.py`
next to `meta.yaml`. The following example are scalings used that work with
shallow clouds using brightness temperature of channels in the "water vapour
window":

```python
# user_functions.py
import numpy as np
import xarray as xr


def normalize(value, lower_limit, upper_limit, clip=True):
    """
    Normalize values between 0 and 1 between a lower and upper limit

    Parameters
    ----------
    value :
        The original value. A single value, vector, or array.
    upper_limit :
        The upper limit.
    lower_limit :
        The lower limit.
    clip : bool
        - True: Clips values between 0 and 1 for RGB.
        - False: Retain the numbers that extends outside 0-1 range.
    Output:
        Values normalized between the upper and lower limit.
    """
    norm = (value - lower_limit) / (upper_limit - lower_limit)
    if clip:
        norm = np.clip(norm, 0, 1)
    return norm


def ir_shallow_clouds_ch11_ch14_ch15_v1(da_bt_11, da_bt_14, da_bt_15):
    da_scene = xr.concat([da_bt_11, da_bt_14, da_bt_15], dim="channel")
    bt_min, bt_max = 270, 300
    clip = True

    def brightness_temperature_to_color(bt):
        return 1.0 - normalize(bt, bt_min, bt_max, clip=clip)

    nx = int(da_scene.x.count())
    ny = int(da_scene.y.count())

    arr_img = np.zeros((ny, nx, 3))
    arr_img[..., 0] = brightness_temperature_to_color(da_scene.sel(channel=14))
    da_ch_11_14_avg = 0.5 * (da_scene.sel(channel=11) + da_scene.sel(channel=14))
    arr_img[..., 1] = brightness_temperature_to_color(da_ch_11_14_avg)
    arr_img[..., 2] = brightness_temperature_to_color(da_scene.sel(channel=15))

    return xr.DataArray(arr_img, dims=("y", "x", "rgb"))
```


# Processing pipeline

To facilitate generating huge datasets and processing all data in parallel the
production of datasets for convml_tt has been implement using the
[luigi](https://luigi.readthedocs.io/) pipeline package by creating individual
luigi `Task`s. Each of these tasks serve a different purpose and the ones you
are most likely to use will be described below. You can also read the source
code yourself in `convml_data.pipeline`.

## Generating scene IDs

```bash
$> python -m luigi --module convml_data.pipeline GenerateSceneIDs
```

## Generate cropped data for all scenes

These scenes will be cropped to the bounds defined in `meta.yaml`

```bash
luigi --module convml_data.pipeline GenerateCroppedScenes
luigi --module convml_data.pipeline GenerateCroppedScenes --aux-name cloud_top_height_ceres
```

## Generate regridded data for all scenes

```bash
luigi --module convml_data.pipeline GenerateRegriddedScenes
luigi --module convml_data.pipeline GenerateRegriddedScenes --aux-name cloud_top_height_ceres
```

# Embedding analysis

## Generate tiles

```bash
luigi --module convml_data.pipeline GenerateTiles --tiles-kind {triplets,trajectories} [--aux-name {cloud_top_height_ceres}]
```

## Generate tile embeddings

```bash
luigi --module convml_data.pipeline.embeddings.sampling AggregatedDatasetScenesTileEmbeddings --step-size 30 --model-path embeddings/models/fixednorm-stage-2.torch.pkl --tiles-kind {triplets,trajectories,rect-slidingwindow}
```

### Transforming embeddings

```bash
luigi --module --module convml_data.pipeline.embeddings.sampling AggregatedDatasetScenesTileEmbeddings --model-path embeddings/models/fixednorm-stage-2.torch.pkl --embedding-transform pca --tiles-kind {triplets,trajectories,rect-slidingwindow}
```

## Plotting optical flow trajectories for all scenes

*NB*: not currently working

```bash
$> python -m luigi --module convml_tt.interpretation.rectpred.pipeline.flow PlotAllScenesWithScenePrefixTrajectories
```


# Analysing auxiliary fields with embeddings

```bash
luigi --module convml_data.pipeline.embeddings.aux_fields.data AggregatedDatasetScenesAuxFieldWithEmbeddings --aux-name cloud_top_temperature --tiles-kind rect-slidingwindow --model-path fixednorm-stage-2.torch.pkl --embedding-model-args '{ "step_size": 100 }'
```

Visualisation:

```bash
luigi --module convml_data.pipeline.embeddings.aux_fields.viz ColumnScalarEmbeddingDistPlot --aux-name total_net_radiation --tiles-kind rect-slidingwindow
```
